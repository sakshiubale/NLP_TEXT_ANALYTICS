{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# TSWA_PRACTICAL_6_TEXT_PROCESSING_PERFROMING_ALL_TOGETHER\n",
        "import nltk"
      ],
      "metadata": {
        "id": "JpLS0vX-UYQ_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "data = requests.get('http://www.gutenberg.org/cache/epub/8001/pg8001.html')\n",
        "content = data.content\n",
        "print(content[1000:2200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXcrjbIkXe92",
        "outputId": "a4726ab8-75f8-4ada-88b5-f7d894cc59ac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b': -4em;\\r\\n    margin-left: 4em;\\r\\n    margin-top: 1em;\\r\\n    margin-bottom: 0;\\r\\n    font-size: medium\\r\\n}\\r\\n#pg-header #pg-header-authlist {\\r\\n    all: initial;\\r\\n    margin-top: 0;\\r\\n    margin-bottom: 0;\\r\\n}\\r\\n#pg-header #pg-machine-header strong {\\r\\n    font-weight: normal;\\r\\n}\\r\\n#pg-header #pg-start-separator, #pg-footer #pg-end-separator {\\r\\n    margin-bottom: 3em;\\r\\n    margin-left: 0;\\r\\n    margin-right: auto;\\r\\n    margin-top: 2em;\\r\\n    text-align: center\\r\\n}\\r\\n\\r\\n    .xhtml_center {text-align: center; display: block;}\\r\\n    .xhtml_center table {\\r\\n        display: table;\\r\\n        text-align: left;\\r\\n        margin-left: auto;\\r\\n        margin-right: auto;\\r\\n        }</style><title>The Project Gutenberg eBook of The Bible, King James version, Book 1: Genesis, by Anonymous</title><style>/* ************************************************************************\\r\\n * classless css copied from https://www.pgdp.net/wiki/CSS_Cookbook/Styles\\r\\n * ********************************************************************** */\\r\\n/* ************************************************************************\\r\\n * set the body margins to allow whitespace along sides of window\\r\\n * ******************************************'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1_Removign HTML Tags. Import re\n",
        "import re"
      ],
      "metadata": {
        "id": "txZz7Ov3Xp93"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "tbkwZChhXr-D"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def html_stripping(text):\n",
        "  soup = BeautifulSoup(text, \"html.parser\")\n",
        "  [s.extract() for s in soup(['iframe', 'script'])]\n",
        "  stripped_text = soup.get_text()\n",
        "  stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
        "  return stripped_text"
      ],
      "metadata": {
        "id": "VFVLcAi0XvDh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_content = html_stripping(content)\n",
        "print(clean_content[1036:2045])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZzku6XSX13l",
        "outputId": "0123492d-832b-4236-875a-e3f2e2ba62d0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Book 01        Genesis\n",
            "01:001:001 In the beginning God created the heaven and the earth.\n",
            "01:001:002 And the earth was without form, and void; and darkness was\n",
            "           upon the face of the deep. And the Spirit of God moved upon\n",
            "           the face of the waters.\n",
            "01:001:003 And God said, Let there be light: and there was light.\n",
            "01:001:004 And God saw the light, that it was good: and God divided the\n",
            "           light from the darkness.\n",
            "01:001:005 And God called the light Day, and the darkness he called\n",
            "           Night. And the evening and the morning were the first day.\n",
            "01:001:006 And God said, Let there be a firmament in the midst of the\n",
            "           waters, and let it divide the waters from the waters.\n",
            "01:001:007 And God made the firmament, and divided the waters which were\n",
            "           under the firmament from the waters which were above the\n",
            "           firmament: and it was so.\n",
            "01:001:008 And God called the firmament Heaven. And the evening and the\n",
            "           morning were the second day.\n",
            "01:001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "from pprint import pprint\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "MYS-xvoJX4V6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('all')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nEE2rkFYE80",
        "outputId": "8eaba383-d58f-44cf-84fc-68b0e8b1c3df"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "I9qtzlaKfKDK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a52843d7-596a-4c69-f372-caba3447dfe2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "import re\n",
        "import string\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating our own corpus\n",
        "corpus=[\"The brown fox wasn't quick and he couldn't win the race.\",\n",
        "\"Hey it was a great cricket match @yesterday!!\",\n",
        "\"I just bought a @new mobile for me at $1000.\",\n",
        "\"Python NLP is really ****amazing*****!!@@@\"]"
      ],
      "metadata": {
        "id": "yM2r-aFzUo0x"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_text(text):\n",
        "  sentences=nltk.sent_tokenize(text)\n",
        "  print(sentences)\n",
        "  word_tokens=[nltk.word_tokenize(sentence) for sentence in sentences]\n",
        "  return word_tokens"
      ],
      "metadata": {
        "id": "pN-datxmpgeB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the token list from the above def\n",
        "token_list=[tokenize_text(text) for text in corpus]\n",
        "print(token_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmNp78CMpl-T",
        "outputId": "8981b5ae-00e0-442a-c423-45497b6be89a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"The brown fox wasn't quick and he couldn't win the race.\"]\n",
            "['Hey it was a great cricket match @yesterday!', '!']\n",
            "['I just bought a @new mobile for me at $1000.']\n",
            "['Python NLP is really ****amazing*****!!', '@@@']\n",
            "[[['The', 'brown', 'fox', 'was', \"n't\", 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'race', '.']], [['Hey', 'it', 'was', 'a', 'great', 'cricket', 'match', '@', 'yesterday', '!'], ['!']], [['I', 'just', 'bought', 'a', '@', 'new', 'mobile', 'for', 'me', 'at', '$', '1000', '.']], [['Python', 'NLP', 'is', 'really', '*', '*', '*', '*', 'amazing', '*', '*', '*', '*', '*', '!', '!'], ['@', '@', '@']]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 - Removing special characters\n",
        "def remove_special_characters(sentence,keep_apostrophes=False):\n",
        "  sentence = sentence.strip()\n",
        "  if keep_apostrophes:\n",
        "    PATTERN = r'[?|$|&|*|%|@|(|)|~]' # add other characters here to remove them\n",
        "    filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
        "  else:\n",
        "    PATTERN = r'[^a-zA-Z0-9 ]' # only extract alpha-numeric characters\n",
        "    filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
        "  return filtered_sentence"
      ],
      "metadata": {
        "id": "e2clNxxTZA7d"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence=\"The brown fox wasn't quick and he couldn't win the race.\""
      ],
      "metadata": {
        "id": "g9D5mNkhr3LN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_special_characters(sentence,keep_apostrophes=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vmqq49HQoM-a",
        "outputId": "00e2ddde-2438-4102-c249-acdee4425f9e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The brown fox wasnt quick and he couldnt win the race'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwJprYHEZlNu",
        "outputId": "dc9b2e83-7be9-403a-d7dd-2b278814fab0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import contractions"
      ],
      "metadata": {
        "id": "hmr81PCkZp8z"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contraction_mapping=contractions.contractions_dict\n",
        "print(contraction_mapping)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJNuqzySZtWj",
        "outputId": "0652c201-51c6-4d06-b4b0-9a044b8297b7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"I'm\": 'I am', \"I'm'a\": 'I am about to', \"I'm'o\": 'I am going to', \"I've\": 'I have', \"I'll\": 'I will', \"I'll've\": 'I will have', \"I'd\": 'I would', \"I'd've\": 'I would have', 'Whatcha': 'What are you', \"amn't\": 'am not', \"ain't\": 'are not', \"aren't\": 'are not', \"'cause\": 'because', \"can't\": 'cannot', \"can't've\": 'cannot have', \"could've\": 'could have', \"couldn't\": 'could not', \"couldn't've\": 'could not have', \"daren't\": 'dare not', \"daresn't\": 'dare not', \"dasn't\": 'dare not', \"didn't\": 'did not', 'didn’t': 'did not', \"don't\": 'do not', 'don’t': 'do not', \"doesn't\": 'does not', \"e'er\": 'ever', \"everyone's\": 'everyone is', 'finna': 'fixing to', 'gimme': 'give me', \"gon't\": 'go not', 'gonna': 'going to', 'gotta': 'got to', \"hadn't\": 'had not', \"hadn't've\": 'had not have', \"hasn't\": 'has not', \"haven't\": 'have not', \"he've\": 'he have', \"he's\": 'he is', \"he'll\": 'he will', \"he'll've\": 'he will have', \"he'd\": 'he would', \"he'd've\": 'he would have', \"here's\": 'here is', \"how're\": 'how are', \"how'd\": 'how did', \"how'd'y\": 'how do you', \"how's\": 'how is', \"how'll\": 'how will', \"isn't\": 'is not', \"it's\": 'it is', \"'tis\": 'it is', \"'twas\": 'it was', \"it'll\": 'it will', \"it'll've\": 'it will have', \"it'd\": 'it would', \"it'd've\": 'it would have', 'kinda': 'kind of', \"let's\": 'let us', 'luv': 'love', \"ma'am\": 'madam', \"may've\": 'may have', \"mayn't\": 'may not', \"might've\": 'might have', \"mightn't\": 'might not', \"mightn't've\": 'might not have', \"must've\": 'must have', \"mustn't\": 'must not', \"mustn't've\": 'must not have', \"needn't\": 'need not', \"needn't've\": 'need not have', \"ne'er\": 'never', \"o'\": 'of', \"o'clock\": 'of the clock', \"ol'\": 'old', \"oughtn't\": 'ought not', \"oughtn't've\": 'ought not have', \"o'er\": 'over', \"shan't\": 'shall not', \"sha'n't\": 'shall not', \"shalln't\": 'shall not', \"shan't've\": 'shall not have', \"she's\": 'she is', \"she'll\": 'she will', \"she'd\": 'she would', \"she'd've\": 'she would have', \"should've\": 'should have', \"shouldn't\": 'should not', \"shouldn't've\": 'should not have', \"so've\": 'so have', \"so's\": 'so is', \"somebody's\": 'somebody is', \"someone's\": 'someone is', \"something's\": 'something is', 'sux': 'sucks', \"that're\": 'that are', \"that's\": 'that is', \"that'll\": 'that will', \"that'd\": 'that would', \"that'd've\": 'that would have', \"'em\": 'them', \"there're\": 'there are', \"there's\": 'there is', \"there'll\": 'there will', \"there'd\": 'there would', \"there'd've\": 'there would have', \"these're\": 'these are', \"they're\": 'they are', \"they've\": 'they have', \"they'll\": 'they will', \"they'll've\": 'they will have', \"they'd\": 'they would', \"they'd've\": 'they would have', \"this's\": 'this is', \"this'll\": 'this will', \"this'd\": 'this would', \"those're\": 'those are', \"to've\": 'to have', 'wanna': 'want to', \"wasn't\": 'was not', \"we're\": 'we are', \"we've\": 'we have', \"we'll\": 'we will', \"we'll've\": 'we will have', \"we'd\": 'we would', \"we'd've\": 'we would have', \"weren't\": 'were not', \"what're\": 'what are', \"what'd\": 'what did', \"what've\": 'what have', \"what's\": 'what is', \"what'll\": 'what will', \"what'll've\": 'what will have', \"when've\": 'when have', \"when's\": 'when is', \"where're\": 'where are', \"where'd\": 'where did', \"where've\": 'where have', \"where's\": 'where is', \"which's\": 'which is', \"who're\": 'who are', \"who've\": 'who have', \"who's\": 'who is', \"who'll\": 'who will', \"who'll've\": 'who will have', \"who'd\": 'who would', \"who'd've\": 'who would have', \"why're\": 'why are', \"why'd\": 'why did', \"why've\": 'why have', \"why's\": 'why is', \"will've\": 'will have', \"won't\": 'will not', \"won't've\": 'will not have', \"would've\": 'would have', \"wouldn't\": 'would not', \"wouldn't've\": 'would not have', \"y'all\": 'you all', \"y'all're\": 'you all are', \"y'all've\": 'you all have', \"y'all'd\": 'you all would', \"y'all'd've\": 'you all would have', \"you're\": 'you are', \"you've\": 'you have', \"you'll've\": 'you shall have', \"you'll\": 'you will', \"you'd\": 'you would', \"you'd've\": 'you would have', 'to cause': 'to cause', 'will cause': 'will cause', 'should cause': 'should cause', 'would cause': 'would cause', 'can cause': 'can cause', 'could cause': 'could cause', 'must cause': 'must cause', 'might cause': 'might cause', 'shall cause': 'shall cause', 'may cause': 'may cause', 'jan.': 'january', 'feb.': 'february', 'mar.': 'march', 'apr.': 'april', 'jun.': 'june', 'jul.': 'july', 'aug.': 'august', 'sep.': 'september', 'oct.': 'october', 'nov.': 'november', 'dec.': 'december', 'I’m': 'I am', 'I’m’a': 'I am about to', 'I’m’o': 'I am going to', 'I’ve': 'I have', 'I’ll': 'I will', 'I’ll’ve': 'I will have', 'I’d': 'I would', 'I’d’ve': 'I would have', 'amn’t': 'am not', 'ain’t': 'are not', 'aren’t': 'are not', '’cause': 'because', 'can’t': 'cannot', 'can’t’ve': 'cannot have', 'could’ve': 'could have', 'couldn’t': 'could not', 'couldn’t’ve': 'could not have', 'daren’t': 'dare not', 'daresn’t': 'dare not', 'dasn’t': 'dare not', 'doesn’t': 'does not', 'e’er': 'ever', 'everyone’s': 'everyone is', 'gon’t': 'go not', 'hadn’t': 'had not', 'hadn’t’ve': 'had not have', 'hasn’t': 'has not', 'haven’t': 'have not', 'he’ve': 'he have', 'he’s': 'he is', 'he’ll': 'he will', 'he’ll’ve': 'he will have', 'he’d': 'he would', 'he’d’ve': 'he would have', 'here’s': 'here is', 'how’re': 'how are', 'how’d': 'how did', 'how’d’y': 'how do you', 'how’s': 'how is', 'how’ll': 'how will', 'isn’t': 'is not', 'it’s': 'it is', '’tis': 'it is', '’twas': 'it was', 'it’ll': 'it will', 'it’ll’ve': 'it will have', 'it’d': 'it would', 'it’d’ve': 'it would have', 'let’s': 'let us', 'ma’am': 'madam', 'may’ve': 'may have', 'mayn’t': 'may not', 'might’ve': 'might have', 'mightn’t': 'might not', 'mightn’t’ve': 'might not have', 'must’ve': 'must have', 'mustn’t': 'must not', 'mustn’t’ve': 'must not have', 'needn’t': 'need not', 'needn’t’ve': 'need not have', 'ne’er': 'never', 'o’': 'of', 'o’clock': 'of the clock', 'ol’': 'old', 'oughtn’t': 'ought not', 'oughtn’t’ve': 'ought not have', 'o’er': 'over', 'shan’t': 'shall not', 'sha’n’t': 'shall not', 'shalln’t': 'shall not', 'shan’t’ve': 'shall not have', 'she’s': 'she is', 'she’ll': 'she will', 'she’d': 'she would', 'she’d’ve': 'she would have', 'should’ve': 'should have', 'shouldn’t': 'should not', 'shouldn’t’ve': 'should not have', 'so’ve': 'so have', 'so’s': 'so is', 'somebody’s': 'somebody is', 'someone’s': 'someone is', 'something’s': 'something is', 'that’re': 'that are', 'that’s': 'that is', 'that’ll': 'that will', 'that’d': 'that would', 'that’d’ve': 'that would have', '’em': 'them', 'there’re': 'there are', 'there’s': 'there is', 'there’ll': 'there will', 'there’d': 'there would', 'there’d’ve': 'there would have', 'these’re': 'these are', 'they’re': 'they are', 'they’ve': 'they have', 'they’ll': 'they will', 'they’ll’ve': 'they will have', 'they’d': 'they would', 'they’d’ve': 'they would have', 'this’s': 'this is', 'this’ll': 'this will', 'this’d': 'this would', 'those’re': 'those are', 'to’ve': 'to have', 'wasn’t': 'was not', 'we’re': 'we are', 'we’ve': 'we have', 'we’ll': 'we will', 'we’ll’ve': 'we will have', 'we’d': 'we would', 'we’d’ve': 'we would have', 'weren’t': 'were not', 'what’re': 'what are', 'what’d': 'what did', 'what’ve': 'what have', 'what’s': 'what is', 'what’ll': 'what will', 'what’ll’ve': 'what will have', 'when’ve': 'when have', 'when’s': 'when is', 'where’re': 'where are', 'where’d': 'where did', 'where’ve': 'where have', 'where’s': 'where is', 'which’s': 'which is', 'who’re': 'who are', 'who’ve': 'who have', 'who’s': 'who is', 'who’ll': 'who will', 'who’ll’ve': 'who will have', 'who’d': 'who would', 'who’d’ve': 'who would have', 'why’re': 'why are', 'why’d': 'why did', 'why’ve': 'why have', 'why’s': 'why is', 'will’ve': 'will have', 'won’t': 'will not', 'won’t’ve': 'will not have', 'would’ve': 'would have', 'wouldn’t': 'would not', 'wouldn’t’ve': 'would not have', 'y’all': 'you all', 'y’all’re': 'you all are', 'y’all’ve': 'you all have', 'y’all’d': 'you all would', 'y’all’d’ve': 'you all would have', 'you’re': 'you are', 'you’ve': 'you have', 'you’ll’ve': 'you shall have', 'you’ll': 'you will', 'you’d': 'you would', 'you’d’ve': 'you would have'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3_Replacing contractions with expanded form\n",
        "def contraction_expansion(sentence,contraction_mapping):\n",
        "  contractions_pattern=re.compile('({})'.format('|'.join(contraction_mapping.keys())),flags=re.IGNORECASE|re.DOTALL)"
      ],
      "metadata": {
        "id": "Ua7EsEXlZzq0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To remove the accented characters\n",
        "import unicodedata"
      ],
      "metadata": {
        "id": "C8XZSoyhcFcx"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def for removing accented characters\n",
        "def accented_char_removal(text):\n",
        "  text=str(text)\n",
        "  text = unicodedata.normalize('NFKD',text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "  return text"
      ],
      "metadata": {
        "id": "Y_1fdW76cIXi"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accented_char_removal('Sómě Áccěntěd těxt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RKcTwbxqglKm",
        "outputId": "f5f947a2-dcf8-47b1-9f1c-f1ada5d39fbc"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Some Accented text'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 _ Special Character Removal\n",
        "def special_char_removal(text, remove_digits=False):\n",
        "  pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
        "  text = re.sub(pattern, '', text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "FNdqndd4cVca"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "special_char_removal(\"Well this was fun! What do you think? 123#@!\",remove_digits=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "54-bsBQ4csCt",
        "outputId": "cd73bc7d-b893-4c34-cb60-1c70f2cc3c65"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Well this was fun What do you think '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4_ Stemming\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "_9at4lqedDh_"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()"
      ],
      "metadata": {
        "id": "OtHzyHZ3dE_p"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing stemmer\n",
        "ps.stem('jumping'), ps.stem('jumps'), ps.stem('jumped')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUvJvrZFdHn3",
        "outputId": "cabe8e22-e58a-48ed-b5d0-85be664a0bf3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('jump', 'jump', 'jump')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5_def for stemming\n",
        "def simple_stemmer(text):\n",
        "  ps = nltk.porter.PorterStemmer()\n",
        "  text =' '.join([ps.stem(word) for word in text.split()])\n",
        "  return text"
      ],
      "metadata": {
        "id": "ppRPojcadL96"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simple_stemmer(\"My system keeps crashing his crashed yesterday, ours crashes daily\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mRBFzanZdZW5",
        "outputId": "822758ea-10f4-45f3-afce-dad94332bf85"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'my system keep crash hi crash yesterday, our crash daili'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6_Lemmitization\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "l_iQtRo9dflc"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wnl = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "M96vY_JodmMc"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lemmatize nouns with 'n'\n",
        "print(wnl.lemmatize('cars', 'n')) # s is removed\n",
        "print(wnl.lemmatize('men', 'n'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qY8WdKJ7dufO",
        "outputId": "2ef0ff59-003f-417a-e0d6-7bfd23498e61"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "car\n",
            "men\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lemmatize verbs with 'v'\n",
        "print(wnl.lemmatize('running', 'v')) # extracting the verb from the verb phrase - running\n",
        "print(wnl.lemmatize('ate', 'v')) # verb behind ate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qF_2TuZ9dzjp",
        "outputId": "33b8558b-1bf6-4fa6-81d2-692168afb7c1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n",
            "eat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lemmatize adjectives with 'a'\n",
        "print(wnl.lemmatize('saddest', 'a')) # orginal adjective extracted\n",
        "print(wnl.lemmatize('fancier', 'a'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hE_Jt2mWd6Ol",
        "outputId": "2ab0e41c-e648-481c-9150-3fd0928087d4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sad\n",
            "fancy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6 - def for lemmatization\n",
        "import spacy"
      ],
      "metadata": {
        "id": "baxyrdfceFaK"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "dU0s-o1wenRG"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 - def for lemmatization\n",
        "def text_lemmatization(text):\n",
        "  text = nlp(text)\n",
        "  text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
        "  return text"
      ],
      "metadata": {
        "id": "KLmuD1Qbe_Ly"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_lemmatization('My system keeps crashing! his crashed yesterday, ours crashes daily')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "j-2VvlfqfM3n",
        "outputId": "edd1c34d-4448-405d-8a3a-dfa1bb67e6be"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'my system keep crash ! his crashed yesterday , ours crash daily'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6 - Stopwords removal\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0IHQcqWj6uI",
        "outputId": "6fb1f314-49dd-4a12-938b-c95818cef564"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize.toktok import ToktokTokenizer"
      ],
      "metadata": {
        "id": "0EQN7MOsj8VJ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = ToktokTokenizer()\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "print(stopword_list[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ec_oisFkBf1",
        "outputId": "2326b692-72a3-4db3-8a9d-99f0c6721e6d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6 _ def remove stop words\n",
        "def stopword_removal(tokens):\n",
        "  stopword_list = nltk.corpus.stopwords.words('english')\n",
        "  filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "  filtered_text =''.join(filtered_tokens)\n",
        "  return filtered_text"
      ],
      "metadata": {
        "id": "93rqei12kF7n"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TSWA_PRACTICAL_6_TEXT_PROCESSING_PERFROMING_ALL_TOGETHER_ALTERNATE_SIMPLE_FUNCTION\n",
        "corpus =(\"US unveils world's most powerful supercomputer, beats China. \"\n",
        "\"The US has unveiled the world's most powerful supercomputer called 'Summit', \"\n",
        "\"beating the previous record-holder China's Sunway TaihuLight. With a peak performance \"\n",
        "\"of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, \"\n",
        "\"which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, \"\n",
        "\"which reportedly take up the size of two tennis courts.\")"
      ],
      "metadata": {
        "id": "q2Wt8W8ETacI"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "SMTCnKS_T2G-",
        "outputId": "c6cdd460-d662-4fb3-c36b-66687b801f57"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"US unveils world's most powerful supercomputer, beats China. The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight. With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, which reportedly take up the size of two tennis courts.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_corpus(corpus):\n",
        "  normalized_corpus = []\n",
        "  for text in corpus:\n",
        "    # 1\n",
        "    text=html_stripping(text)\n",
        "    # 2\n",
        "    text=special_char_removal(text)\n",
        "    # 3\n",
        "    text=contraction_expansion(text,contraction_mapping)\n",
        "    # 4\n",
        "    # text=simple_stemmer(text)\n",
        "    # 5\n",
        "    text=accented_char_removal(text)\n",
        "    # 6\n",
        "    text=text_lemmatization(text)\n",
        "    # 7\n",
        "    text=stopword_removal(text)\n",
        "    normalized_corpus.append(text)\n",
        "    #if tokenize:\n",
        "      #text = tokenize_text(text)\n",
        "      #normalized_corpus.append(text)\n",
        "  return normalized_corpus"
      ],
      "metadata": {
        "id": "bw6f_ZI_uhVv"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalize_corpus(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doclljwKxbBx",
        "outputId": "21b5d1dd-fcf1-4404-97c8-f4dd99a48362"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne',\n",
              " 'nne']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Text Normalizer for extracting the newsgoup data set\n",
        "!pip install text_normalizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quVqkLbir17c",
        "outputId": "fb5b7096-2301-465a-aaae-8ceb60aedf78"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting text_normalizer\n",
            "  Downloading text-normalizer-0.1.3.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from text_normalizer) (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->text_normalizer) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->text_normalizer) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->text_normalizer) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->text_normalizer) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->text_normalizer) (1.16.0)\n",
            "Building wheels for collected packages: text_normalizer\n",
            "  Building wheel for text_normalizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for text_normalizer: filename=text_normalizer-0.1.3-cp310-cp310-linux_x86_64.whl size=247443 sha256=712a9d7c8d36c70788ee19e98fd7cd75dfe2e8d8743aeb9e0b5af10a6be668a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/9f/80/4a4e7d2d6f6fc35b19993353c2c8f1f7ac48ac29c826d2e676\n",
            "Successfully built text_normalizer\n",
            "Installing collected packages: text_normalizer\n",
            "Successfully installed text_normalizer-0.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import text_normalizer as tn"
      ],
      "metadata": {
        "id": "DPt2BLsNlQbf"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_corpus(corpus,html_stripping=html_stripping,contraction_expansion=contraction_expansion,accented_char_removal=accented_char_removal,\n",
        "                     text_lower_case=True,text_lemmatization=True,special_char_removal=True,\n",
        "                     stopword_removal=True,remove_digits=True):\n",
        "  normalize_corpus2=[]\n",
        "  # normalize each document in the corpus\n",
        "  for doc in corpus:\n",
        "    # strip HTML\n",
        "    if html_stripping:\n",
        "      doc=html_stripping(doc)\n",
        "      print(doc)\n",
        "      # remove accented characters\n",
        "    if accented_char_removal:\n",
        "      doc=accented_char_removal(doc)\n",
        "      print(doc)\n",
        "      # expand contractions\n",
        "    if contraction_expansion:\n",
        "      doc=contraction_expansion(doc)\n",
        "      print(doc)\n",
        "      # lowercase the text\n",
        "    if text_lower_case:\n",
        "      doc = doc.lower()\n",
        "      # remove extra newlines\n",
        "      doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
        "      print(doc)\n",
        "      # lemmatize text\n",
        "    if text_lemmatization:\n",
        "      doc = text_lemmatization(doc)\n",
        "      print(doc)\n",
        "      # remove special characters and\\or digits\n",
        "    if special_char_removal:\n",
        "      # insert spaces between special characters to isolate them\n",
        "      special_char_pattern = re.compile(r'([{.(-)!}])')\n",
        "      doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
        "      doc = remove_special_characters(doc, remove_digits=remove_digits)\n",
        "      # remove extra whitespace\n",
        "      doc = re.sub(' +', ' ', doc)\n",
        "      print(doc)\n",
        "      # remove stopwords\n",
        "    if stopword_removal:\n",
        "      doc = stopword_removal(doc,is_lower_case=text_lower_case)\n",
        "      print(doc)\n",
        "    normalize_corpus.append(doc)\n",
        "    print(doc)\n",
        "  return(normalize_corpus)"
      ],
      "metadata": {
        "id": "d42TEZYJN31u"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups"
      ],
      "metadata": {
        "id": "QPUWf7fDMWhL"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create objects for each package\n",
        "import numpy as np\n",
        "import text_normalizer as tn\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Pfu510qpMgJn"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch data\n",
        "data=fetch_20newsgroups(subset='all')\n",
        "data = fetch_20newsgroups(subset='all', shuffle=True,remove=('headers', 'footers', 'quotes'))\n",
        "data_labels_map = dict(enumerate(data.target_names))"
      ],
      "metadata": {
        "id": "qbyXeBV_giZ3"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# building the dataframe for the data extracted from newgroups\n",
        "# Create a corpus of newsgroup sentences\n",
        "corpus=data.data\n",
        "target_labels=data.target\n",
        "target_names = [data_labels_map[label] for label in data.target]\n",
        "data_df = pd.DataFrame({'Article': corpus, 'Target Label': target_labels,'Target Name': target_names})\n",
        "print(data_df.shape)\n",
        "data_df.head(10)"
      ],
      "metadata": {
        "id": "X-82brBxgsL9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "cbfb3750-48c7-49b9-afc7-7f1ef6c2a47c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18846, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             Article  Target Label  \\\n",
              "0  \\n\\nI am sure some bashers of Pens fans are pr...            10   \n",
              "1  My brother is in the market for a high-perform...             3   \n",
              "2  \\n\\n\\n\\n\\tFinally you said what you dream abou...            17   \n",
              "3  \\nThink!\\n\\nIt's the SCSI card doing the DMA t...             3   \n",
              "4  1)    I have an old Jasmine drive which I cann...             4   \n",
              "5  \\n\\nBack in high school I worked as a lab assi...            12   \n",
              "6  \\n\\nAE is in Dallas...try 214/241-6060 or 214/...             4   \n",
              "7  \\n[stuff deleted]\\n\\nOk, here's the solution t...            10   \n",
              "8  \\n\\n\\nYeah, it's the second one.  And I believ...            10   \n",
              "9  \\nIf a Christian means someone who believes in...            19   \n",
              "\n",
              "                Target Name  \n",
              "0          rec.sport.hockey  \n",
              "1  comp.sys.ibm.pc.hardware  \n",
              "2     talk.politics.mideast  \n",
              "3  comp.sys.ibm.pc.hardware  \n",
              "4     comp.sys.mac.hardware  \n",
              "5           sci.electronics  \n",
              "6     comp.sys.mac.hardware  \n",
              "7          rec.sport.hockey  \n",
              "8          rec.sport.hockey  \n",
              "9        talk.religion.misc  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bbc4c7c2-c54a-4261-9cdb-f7db2ec8532a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Article</th>\n",
              "      <th>Target Label</th>\n",
              "      <th>Target Name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n",
              "      <td>10</td>\n",
              "      <td>rec.sport.hockey</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>My brother is in the market for a high-perform...</td>\n",
              "      <td>3</td>\n",
              "      <td>comp.sys.ibm.pc.hardware</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\\n\\n\\n\\n\\tFinally you said what you dream abou...</td>\n",
              "      <td>17</td>\n",
              "      <td>talk.politics.mideast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\\nThink!\\n\\nIt's the SCSI card doing the DMA t...</td>\n",
              "      <td>3</td>\n",
              "      <td>comp.sys.ibm.pc.hardware</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1)    I have an old Jasmine drive which I cann...</td>\n",
              "      <td>4</td>\n",
              "      <td>comp.sys.mac.hardware</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>\\n\\nBack in high school I worked as a lab assi...</td>\n",
              "      <td>12</td>\n",
              "      <td>sci.electronics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>\\n\\nAE is in Dallas...try 214/241-6060 or 214/...</td>\n",
              "      <td>4</td>\n",
              "      <td>comp.sys.mac.hardware</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>\\n[stuff deleted]\\n\\nOk, here's the solution t...</td>\n",
              "      <td>10</td>\n",
              "      <td>rec.sport.hockey</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>\\n\\n\\nYeah, it's the second one.  And I believ...</td>\n",
              "      <td>10</td>\n",
              "      <td>rec.sport.hockey</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>\\nIf a Christian means someone who believes in...</td>\n",
              "      <td>19</td>\n",
              "      <td>talk.religion.misc</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bbc4c7c2-c54a-4261-9cdb-f7db2ec8532a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bbc4c7c2-c54a-4261-9cdb-f7db2ec8532a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bbc4c7c2-c54a-4261-9cdb-f7db2ec8532a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-bbe0647e-50da-4e84-918d-18d205236ef6\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bbe0647e-50da-4e84-918d-18d205236ef6')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-bbe0647e-50da-4e84-918d-18d205236ef6 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data_df",
              "summary": "{\n  \"name\": \"data_df\",\n  \"rows\": 18846,\n  \"fields\": [\n    {\n      \"column\": \"Article\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 18287,\n        \"samples\": [\n          \"I remember someone mention about a 150meg syquest.  Has anyone else\\nheard anything about this?  I'd be interested in the cost per megabyte and the\\napproximate cost of the drive itself and how they compare to the Bernoulli 150.\\n\",\n          \": >Atoms are not objective.  They aren't even real.  What scientists call\\n: >an atom is nothing more than a mathematical model that describes \\n: >certain physical, observable properties of our surroundings.  All\\n: >of which is subjective.  \\n: >\\n: >-jim halat\\n\\n: This deserves framing.  It really does.  \\\"[Atoms] aren't even real.\\\"\\n\\n: Tell me then, those atoms we have seen with electron microscopes are\\n: atoms now, so what are they?  Figments of our imaginations?  The\\n: evidence that atoms are real is overwhelming, but I won't bother with\\n: most evidence at the moment.\\n\\nYou would have us believe that what the eye perceives as images are\\nactaully there - as perceived? This may be interesting. I thought\\nthat an electron microscope was used because no wavelength of \\\"light\\\"\\ncan illuminate any \\\"object\\\" of atomic scale. If this image is to have\\nuseful resolution, wouldn't the illuminating sources wavelength have\\nto be several orders of magnitude less than size of thing observed?\\n\\nIf an atom is a \\\"probablity cloud\\\", lower resolutions would give the\\nappearance of solidity, but it seems fairly certain that an atom is\\nnot an object is any conventional sense. Obviously I am not a\\nphysicist, but the question does have ramification of a philosophic\\nnature. Anyway, just a stray thought, carry on ...\",\n          \"\\n\\nThat's a good way to put it.  It's a quiet confidence.  I don't feel like\\nI have to defend this team anymore. \\n\\nThe Devils are a puzzle to me.  They have long been one of the few teams\\nI always dreaded playing.  Their D isn't quite what it used to be but they\\nstill usually play us pretty tough.  I just can't figure it out.  At least\\nlast night they mounted an attack.\\n\\n\\nBernie Nichols was interviewed after Game 2 on the late news.  He was\\nshaking his head in disbelief over Barrasso, saying that Tommy is like\\nanother defenseman out there and how difficult it is playing against him.\\nThen he got this far-away look on his face, his voice started trailing\\noff and he said, \\\"It just isn't right...it just isn't fair...\\\" and he\\ntried to muster a smile.\\n\\n\\nAs far as coaching goes, a Pens/Islanders match-up should be pretty good.\\n\\nOne guy I feel sorry for in all of this so far is Chris Terreri.  I'm sure\\nTom Barrasso knows how he feels.  For so long Tommy would look up and not\\nhave a teammate in sight.  Terreri's teammates really left him out in the\\ncold in the first two games.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Target Label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5,\n        \"min\": 0,\n        \"max\": 19,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          10,\n          18,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Target Name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"rec.sport.hockey\",\n          \"talk.politics.misc\",\n          \"misc.forsale\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this dataset, we can see that each document has some textual content and the\n",
        "label can be denoted by a specific number, which maps to a newsgroup category name"
      ],
      "metadata": {
        "id": "pfMDTvZbkH1f"
      }
    }
  ]
}